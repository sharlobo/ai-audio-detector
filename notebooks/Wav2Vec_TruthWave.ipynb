{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec2 Model \n",
    "\n",
    "## Environment Setup \n",
    "\n",
    "* Clean and isolated Python environment in order to manage packages and avoid conflicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating virtual enviornment\n",
    "!python3 -m venv wavenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual environment activated\r\n"
     ]
    }
   ],
   "source": [
    "# activating wavnev enviornment - anything ran inside will stay inside\n",
    "!source wavenv/bin/activate && echo \"Virtual environment activated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in ./wavenv/lib/python3.9/site-packages (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (1.8.14)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (8.18.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./wavenv/lib/python3.9/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in ./wavenv/lib/python3.9/site-packages (from ipykernel) (24.2)\n",
      "Requirement already satisfied: psutil in ./wavenv/lib/python3.9/site-packages (from ipykernel) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./wavenv/lib/python3.9/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: stack-data in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.13.2)\n",
      "Requirement already satisfied: exceptiongroup in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./wavenv/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in ./wavenv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./wavenv/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./wavenv/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.7)\n",
      "Requirement already satisfied: zipp>=3.20 in ./wavenv/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.21.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./wavenv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./wavenv/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./wavenv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in ./wavenv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./wavenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./wavenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./wavenv/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "/bin/bash: -c: line 0: unexpected EOF while looking for matching `\"'\n",
      "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "# installs so jupyter can recognize kernel option \n",
    "!wavenv/bin/pip install ipykernel\n",
    "!wavenv/bin/python -m ipykernel install --user --name=wavenv --display-name \"Python (wavenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/stor/home/spl742/wavenv/bin/python\n"
     ]
    }
   ],
   "source": [
    "#checking python running from my virtual environment \n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/rocm5.7\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/rocm5.7/torch-2.3.1%2Brocm5.7-cp39-cp39-linux_x86_64.whl (1905.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 GB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0mm\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/rocm5.7/torchvision-0.18.1%2Brocm5.7-cp39-cp39-linux_x86_64.whl (65.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/rocm5.7/torchaudio-2.3.1%2Brocm5.7-cp39-cp39-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m226.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pytorch-triton-rocm==2.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/pytorch_triton_rocm-2.3.1-cp39-cp39-linux_x86_64.whl (234.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.3/234.3 MB\u001b[0m \u001b[31m209.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/numpy-1.26.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m214.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
      "  Downloading https://download.pytorch.org/whl/pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of networkx to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m157.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m217.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/pillow-11.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m213.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m123.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, pytorch-triton-rocm, jinja2, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'LICENSE'\n",
      "Check the permissions.\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# installing packages built for ROCm 5.7\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 8\n",
      "Device name: AMD Instinct MI100\n"
     ]
    }
   ],
   "source": [
    "# GPU check\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# library imports \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import glob\n",
    "\n",
    "# device check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2Vec2\n",
    "\n",
    "Wav2Vec is a pre-trained model developed by Facebook AI. Especially designed to extract meaningful features from raw audio without manual feature eng. Including:\n",
    "\n",
    "* Self Supervised Learning on Speech: trained on real human speech data. Able to pickup subtle patterns in how real people speak. \n",
    "\n",
    "* Feature Extraction: Is able to output high deimensional embeddings -- strong input features for a classifier\n",
    "\n",
    "* Transfer Learning: No fine tuning\n",
    "\n",
    "* Detection: Wav2Vec can help identify patterns or rythm, Wav2Vec2 helps uncover details \n",
    "\n",
    "* Block loads pre-trained Wav2Vec model and sets everything to run in CPU:\n",
    "\n",
    "* processor = loads feature extractor/tokenizer that matches the pre-trained model. Processor handles converting raw audio into a format for Wav2Vec \n",
    "\n",
    "* model = loading model weights -- trained on 960 hrs of unlabeled speech data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Wav2Vec2 model \n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h\").cpu()  # Force CPU\n",
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unzipping audio data \n",
    "\n",
    "# ZIP files into folders\n",
    "zip_path = 'FAKE.zip'\n",
    "real_zip_path = 'REAL.zip'\n",
    "extract_path = 'fake_audio/'\n",
    "\n",
    "# directory check\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "with zipfile.ZipFile(real_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging .wav files in: obama-original\n",
      "Saved: fake_audio/REAL/obama-original_merged.wav\n",
      "Merging .wav files in: trump-original\n",
      "Saved: fake_audio/REAL/trump-original_merged.wav\n",
      "Merging .wav files in: biden-original\n",
      "Saved: fake_audio/REAL/biden-original_merged.wav\n",
      "Merging .wav files in: ryan-original\n",
      "Saved: fake_audio/REAL/ryan-original_merged.wav\n",
      "Merging .wav files in: musk-original\n",
      "Saved: fake_audio/REAL/musk-original_merged.wav\n",
      "Merging .wav files in: linus-original\n",
      "Saved: fake_audio/REAL/linus-original_merged.wav\n",
      "Merging .wav files in: taylor-original\n",
      "Saved: fake_audio/REAL/taylor-original_merged.wav\n",
      "Merging .wav files in: margot-original\n",
      "Saved: fake_audio/REAL/margot-original_merged.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# merging audio segments for each speaker\n",
    "\n",
    "base_path = 'fake_audio/REAL'  \n",
    "\n",
    "# loop through each folder\n",
    "# collecting all .wav files inside \n",
    "\n",
    "for folder_name in os.listdir(base_path):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Merging .wav files in: {folder_name}\")\n",
    "        combined_audio = []\n",
    "        sr = None\n",
    "\n",
    "        wav_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.wav')])\n",
    "\n",
    "        # librosa - loading each file and appending the audio data to list \n",
    "        for wav_file in wav_files:\n",
    "            wav_path = os.path.join(folder_path, wav_file)\n",
    "            y, sr = librosa.load(wav_path, sr=None)\n",
    "            combined_audio.append(y)\n",
    "\n",
    "        if combined_audio:\n",
    "            merged_audio = np.concatenate(combined_audio) # merging clips into one wav\n",
    "            output_path = os.path.join(base_path, f\"{folder_name}_merged.wav\")\n",
    "            sf.write(output_path, merged_audio, sr) # save into new .wav - using soundfile\n",
    "            print(f\"Saved: {output_path}\") \n",
    "        else:\n",
    "            print(f\"No .wav files in {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging .wav files in: musk-to-taylor\n",
      "Saved: fake_audio/FAKE/musk-to-taylor_merged.wav\n",
      "Merging .wav files in: obama-to-taylor\n",
      "Saved: fake_audio/FAKE/obama-to-taylor_merged.wav\n",
      "Merging .wav files in: Trump\n",
      "No .wav files in Trump\n",
      "Merging .wav files in: musk-to-obama\n",
      "Saved: fake_audio/FAKE/musk-to-obama_merged.wav\n",
      "Merging .wav files in: taylor-to-biden\n",
      "Saved: fake_audio/FAKE/taylor-to-biden_merged.wav\n",
      "Merging .wav files in: biden-to-ryan\n",
      "Saved: fake_audio/FAKE/biden-to-ryan_merged.wav\n",
      "Merging .wav files in: biden-to-Obama\n",
      "Saved: fake_audio/FAKE/biden-to-Obama_merged.wav\n",
      "Merging .wav files in: linus-to-margot\n",
      "Saved: fake_audio/FAKE/linus-to-margot_merged.wav\n",
      "Merging .wav files in: trump-to-musk\n",
      "Saved: fake_audio/FAKE/trump-to-musk_merged.wav\n",
      "Merging .wav files in: musk-to-linus\n",
      "Saved: fake_audio/FAKE/musk-to-linus_merged.wav\n",
      "Merging .wav files in: ryan-to-trump\n",
      "Saved: fake_audio/FAKE/ryan-to-trump_merged.wav\n",
      "Merging .wav files in: taylor-to-obama\n",
      "Saved: fake_audio/FAKE/taylor-to-obama_merged.wav\n",
      "Merging .wav files in: trump-to-ryan\n",
      "Saved: fake_audio/FAKE/trump-to-ryan_merged.wav\n",
      "Merging .wav files in: biden-to-musk\n",
      "Saved: fake_audio/FAKE/biden-to-musk_merged.wav\n",
      "Merging .wav files in: musk-to-margot\n",
      "Saved: fake_audio/FAKE/musk-to-margot_merged.wav\n",
      "Merging .wav files in: Obama-to-Biden\n",
      "Saved: fake_audio/FAKE/Obama-to-Biden_merged.wav\n",
      "Merging .wav files in: margot-to-biden\n",
      "Saved: fake_audio/FAKE/margot-to-biden_merged.wav\n",
      "Merging .wav files in: ryan-to-margot\n",
      "Saved: fake_audio/FAKE/ryan-to-margot_merged.wav\n",
      "Merging .wav files in: musk-to-trump\n",
      "Saved: fake_audio/FAKE/musk-to-trump_merged.wav\n",
      "Merging .wav files in: taylor-to-musk\n",
      "Saved: fake_audio/FAKE/taylor-to-musk_merged.wav\n",
      "Merging .wav files in: margot-to-musk\n",
      "Saved: fake_audio/FAKE/margot-to-musk_merged.wav\n",
      "Merging .wav files in: trump-to-taylor\n",
      "Saved: fake_audio/FAKE/trump-to-taylor_merged.wav\n",
      "Merging .wav files in: obama-to-ryan\n",
      "Saved: fake_audio/FAKE/obama-to-ryan_merged.wav\n",
      "Merging .wav files in: linus-to-musk\n",
      "Saved: fake_audio/FAKE/linus-to-musk_merged.wav\n",
      "Merging .wav files in: biden-to-Trump\n",
      "Saved: fake_audio/FAKE/biden-to-Trump_merged.wav\n",
      "Merging .wav files in: obama-to-musk\n",
      "Saved: fake_audio/FAKE/obama-to-musk_merged.wav\n",
      "Merging .wav files in: obama-to-linus\n",
      "Saved: fake_audio/FAKE/obama-to-linus_merged.wav\n",
      "Merging .wav files in: ryan-to-biden\n",
      "Saved: fake_audio/FAKE/ryan-to-biden_merged.wav\n",
      "Merging .wav files in: margot-to-trump\n",
      "Saved: fake_audio/FAKE/margot-to-trump_merged.wav\n",
      "Merging .wav files in: linus-to-trump\n",
      "Saved: fake_audio/FAKE/linus-to-trump_merged.wav\n",
      "Merging .wav files in: Obama-to-Trump\n",
      "Saved: fake_audio/FAKE/Obama-to-Trump_merged.wav\n"
     ]
    }
   ],
   "source": [
    "# merging process repeated, to combine fake audio segments into single .wav files \n",
    "\n",
    "base_path = 'fake_audio/FAKE'  # for FAKE folder\n",
    "\n",
    "for folder_name in os.listdir(base_path):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Merging .wav files in: {folder_name}\")\n",
    "        combined_audio = []\n",
    "        sr = None\n",
    "\n",
    "        wav_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.wav')])\n",
    "\n",
    "        for wav_file in wav_files:\n",
    "            wav_path = os.path.join(folder_path, wav_file)\n",
    "            y, sr = librosa.load(wav_path, sr=None)\n",
    "            combined_audio.append(y)\n",
    "\n",
    "        if combined_audio:\n",
    "            merged_audio = np.concatenate(combined_audio)\n",
    "            output_path = os.path.join(base_path, f\"{folder_name}_merged.wav\")\n",
    "            sf.write(output_path, merged_audio, sr)\n",
    "            print(f\"Saved: {output_path}\")\n",
    "        else:\n",
    "            print(f\"No .wav files in {folder_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2Vec2 feature extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes a single .wav audio file and extracts features using the Wav2Vec2 model\n",
    "\n",
    "def extract_features(file_path, max_duration_sec=10):\n",
    "    try:\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        \n",
    "        # loading audio waveform + sample rate\n",
    "        audio_input, sample_rate = torchaudio.load(file_path)\n",
    "        print(f\"Loaded audio: {audio_input.shape}, Sample rate: {sample_rate}\")\n",
    "\n",
    "        # truncate long files - consistent inputs\n",
    "        max_len = int(sample_rate * max_duration_sec)\n",
    "        if audio_input.shape[1] > max_len:\n",
    "            audio_input = audio_input[:, :max_len]\n",
    "\n",
    "        # convert stereo to mono if audio has more than one channel\n",
    "        if audio_input.shape[0] > 1:\n",
    "            audio_input = audio_input.mean(dim=0, keepdim=True)\n",
    "            print(\"Converted to mono\")\n",
    "\n",
    "        # resample to 16kHz - wav2vec2 expected input\n",
    "        if sample_rate != 16000:\n",
    "            print(f\"Resampling from {sample_rate} to 16000 Hz\")\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            audio_input = resampler(audio_input)\n",
    "\n",
    "        # preparing model input for wav2vec2 \n",
    "        input_values = processor(audio_input.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = {k: v.to(\"cpu\") for k, v in input_values.items()}\n",
    "        print(\"Prepared input for model\")\n",
    "\n",
    "        # extracting features \n",
    "        # output: hidden states - averaged over time, produces a single feature vector \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input_values)\n",
    "            features = outputs.last_hidden_state\n",
    "            print(\"Features extracted successfully\")\n",
    "\n",
    "        # feature vector - numpy array (summary of audio characteristics)\n",
    "        return features.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}:\", e)\n",
    "        return None\n",
    "\n",
    "    # included print statements for debugging! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: fake_audio/FAKE/biden-to-Trump/segment_2414.wav\n",
      "Loaded audio: torch.Size([2, 200000]), Sample rate: 40000\n",
      "Converted to mono\n",
      "Resampling from 40000 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Feature shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# test block \n",
    "\n",
    "# specific block \n",
    "test_path = \"fake_audio/FAKE/biden-to-Trump/segment_2414.wav\"\n",
    "feature = extract_features(test_path) # feature extraction\n",
    "\n",
    "# output\n",
    "if feature is not None:\n",
    "    print(\"Feature shape:\", feature.shape)\n",
    "else:\n",
    "    print(\"Feature extraction failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already processed: features/real/obama-original_segment_5660.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5667.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5615.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5669.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5731.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5703.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5627.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5620.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5704.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5683.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5652.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5684.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5655.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5629.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5644.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5695.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5638.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5643.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5692.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5631.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5715.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5712.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5636.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5727.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5720.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5678.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5676.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5729.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5671.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5685.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5654.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5628.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5682.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5653.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5621.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5705.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5702.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5626.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5613.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5614.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5668.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5730.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5666.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5661.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5728.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5670.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5677.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5721.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5679.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5726.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5713.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5637.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5630.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5714.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5642.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5693.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5645.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5694.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5639.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5674.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5673.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5725.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5722.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5633.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5717.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5710.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5648.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5699.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5634.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5646.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5697.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5719.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5641.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5690.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5681.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5650.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5708.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5686.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5657.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5688.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5659.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5701.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5625.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5622.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5706.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5617.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5733.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5662.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5619.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5665.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5718.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5640.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5691.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5647.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5696.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5711.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5649.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5698.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5635.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5632.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5716.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5723.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5724.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5672.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5675.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5618.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5664.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5663.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5616.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5732.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5623.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5707.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5689.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5658.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5700.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5624.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5687.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5656.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5680.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5651.npy\n",
      "Skipping already processed: features/real/obama-original_segment_5709.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5434.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5393.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5448.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5394.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5433.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5441.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5490.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5446.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5379.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5473.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5474.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5408.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5406.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5377.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5401.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5410.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5417.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5465.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5419.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5462.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5486.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5457.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5481.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5450.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5422.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5385.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5382.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5425.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5488.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5459.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5376.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5400.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5371.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5407.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5475.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5409.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5378.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5472.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5447.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5440.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5491.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5432.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5395.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5392.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5435.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5449.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5424.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5383.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5489.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5458.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5384.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5423.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5480.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5451.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5487.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5456.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5463.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5464.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5418.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5416.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5411.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5387.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5420.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5427.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5380.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5429.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5484.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5455.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5483.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5452.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5389.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5467.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5460.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5412.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5469.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5415.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5478.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5372.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5404.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5375.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5403.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5471.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5476.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5443.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5398.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5438.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5444.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5391.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5436.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5431.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5396.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5468.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5414.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5413.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5461.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5466.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5482.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5453.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5388.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5428.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5485.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5454.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5381.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5426.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5421.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5386.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5397.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5430.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5437.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5390.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5439.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5445.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5442.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5399.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5477.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5470.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5374.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5402.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5479.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5373.npy\n",
      "Skipping already processed: features/real/trump-original_segment_5405.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5325.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5359.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5322.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5350.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5274.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5273.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5357.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5297.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5362.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5365.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5319.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5290.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5317.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5310.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5299.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5288.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5259.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5301.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5306.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5281.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5250.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5308.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5286.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5257.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5262.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5346.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5341.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5265.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5333.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5334.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5348.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5311.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5298.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5316.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5364.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5318.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5291.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5296.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5363.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5272.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5356.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5351.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5275.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5323.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5324.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5358.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5335.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5349.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5332.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5340.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5264.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5263.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5347.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5287.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5256.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5280.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5251.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5309.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5307.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5289.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5258.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5300.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5269.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5331.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5336.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5260.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5338.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5344.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5343.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5267.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5283.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5252.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5284.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5255.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5303.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5304.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5369.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5315.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5312.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5295.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5360.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5367.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5292.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5352.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5276.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5329.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5271.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5355.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5327.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5320.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5278.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5305.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5302.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5285.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5254.npy\n",
      "Processing: fake_audio/REAL/biden-original/segment_5370.wav\n",
      "Loaded audio: torch.Size([2, 144]), Sample rate: 48000\n",
      "Converted to mono\n",
      "Resampling from 48000 to 16000 Hz\n",
      "Prepared input for model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing fake_audio/REAL/biden-original/segment_5370.wav: Calculated padded input size per channel: (1). Kernel size: (3). Kernel size can't be greater than actual input size\n",
      "Skipping already processed: features/real/biden-original_segment_5282.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5253.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5342.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5266.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5261.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5339.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5345.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5337.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5268.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5330.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5321.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5279.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5326.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5328.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5270.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5354.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5353.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5277.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5366.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5293.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5294.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5361.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5313.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5368.npy\n",
      "Skipping already processed: features/real/biden-original_segment_5314.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_6003.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_6004.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5993.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5994.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5995.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5992.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_6002.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5998.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5991.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5996.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_6001.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5987.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5989.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_6000.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5997.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5990.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5999.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5988.npy\n",
      "Skipping already processed: features/real/ryan-original_segment_5986.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5891.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5918.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5964.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5963.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5896.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5898.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5911.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5916.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5923.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5958.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5924.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5956.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5872.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5875.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5980.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5951.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5940.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5947.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5949.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5935.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5932.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5907.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5900.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5889.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5972.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5887.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5909.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5880.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5975.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5874.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5981.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5950.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5957.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5873.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5959.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5925.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5922.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5917.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5899.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5910.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5962.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5897.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5890.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5919.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5965.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5908.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5881.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5974.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5973.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5886.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5901.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5888.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5906.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5933.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5948.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5934.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5946.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5865.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5941.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5970.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5885.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5882.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5977.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5905.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5979.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5902.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5937.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5930.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5868.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5866.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5942.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5945.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5939.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5985.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5954.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5870.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5928.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5877.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5982.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5953.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5879.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5921.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5926.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5913.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5914.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5968.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5893.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5966.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5961.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5894.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5944.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5938.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5867.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5943.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5931.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5869.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5936.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5903.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5904.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5978.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5883.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5976.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5971.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5884.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5960.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5895.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5892.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5967.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5915.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5969.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5912.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5927.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5878.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5920.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5876.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5983.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5952.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5984.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5955.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5871.npy\n",
      "Skipping already processed: features/real/musk-original_segment_5929.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5786.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5832.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5757.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5781.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5835.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5750.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5849.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5847.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5840.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5788.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5759.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5809.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5765.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5800.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5762.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5807.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5816.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5773.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5811.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5774.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5863.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5864.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5818.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5851.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5799.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5856.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5790.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5824.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5858.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5797.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5823.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5763.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5806.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5764.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5801.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5808.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5841.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5789.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5758.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5846.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5780.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5834.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5751.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5848.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5787.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5833.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5756.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5796.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5822.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5791.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5825.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5859.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5857.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5850.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5798.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5819.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5862.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5810.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5775.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5817.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5772.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5826.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5792.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5821.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5795.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5853.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5828.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5854.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5778.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5861.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5814.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5771.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5813.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5776.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5767.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5802.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5760.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5805.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5769.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5839.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5845.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5842.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5830.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5784.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5755.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5837.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5783.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5752.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5812.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5777.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5815.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5770.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5779.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5860.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5829.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5855.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5852.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5820.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5794.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5827.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5793.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5836.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5782.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5753.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5831.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5785.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5754.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5843.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5838.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5844.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5768.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5761.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5804.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5766.npy\n",
      "Skipping already processed: features/real/linus-original_segment_5803.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5526.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5521.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5582.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5553.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5528.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5585.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5554.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5494.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5561.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5566.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5493.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5568.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5514.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5603.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5513.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5604.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5502.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5579.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5612.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5505.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5577.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5570.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5539.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5545.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5594.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5542.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5593.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5530.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5537.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5512.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5605.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5569.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5515.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5602.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5567.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5492.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5495.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5560.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5529.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5584.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5555.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5583.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5552.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5520.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5527.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5536.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5531.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5543.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5592.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5538.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5544.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5595.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5571.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5576.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5578.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5504.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5503.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5532.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5535.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5549.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5598.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5547.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5596.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5540.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5591.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5575.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5509.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5572.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5500.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5610.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5507.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5516.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5601.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5498.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5511.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5606.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5496.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5608.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5563.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5564.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5518.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5580.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5551.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5587.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5556.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5524.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5589.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5558.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5523.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5611.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5506.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5501.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5573.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5574.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5508.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5541.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5590.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5546.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5597.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5534.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5548.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5599.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5533.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5522.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5525.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5588.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5559.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5586.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5557.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5581.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5550.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5565.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5519.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5497.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5609.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5562.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5499.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5510.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5607.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5517.npy\n",
      "Skipping already processed: features/real/taylor-original_segment_5600.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5746.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5741.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5748.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5734.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5749.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5735.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5740.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5747.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5736.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5744.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5738.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5743.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5742.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5745.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5739.npy\n",
      "Skipping already processed: features/real/margot-original_segment_5737.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4975.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4909.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4880.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4887.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4972.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4900.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4889.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4907.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4932.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4935.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4949.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4998.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4947.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4996.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4940.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4991.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4980.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4951.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4987.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4956.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4924.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4989.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4958.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4923.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4916.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4898.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4911.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4896.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4963.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4964.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4891.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4918.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4941.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4990.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4946.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4997.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4934.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4948.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4933.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4906.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4901.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4888.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4886.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4973.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4974.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4908.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4881.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4965.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4890.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4919.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4897.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4962.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4899.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4910.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4917.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4922.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4925.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4988.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4959.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4986.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4957.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4981.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4950.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4894.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4961.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4966.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4893.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4968.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4914.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4913.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4926.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4879.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4921.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4982.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4953.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4928.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4985.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4954.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4939.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4945.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4994.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4942.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4993.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4930.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4937.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4902.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4979.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4905.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4977.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4882.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4885.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4970.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4929.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4984.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4955.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4983.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4952.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4920.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4927.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4912.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4969.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4915.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4967.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4892.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4895.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4960.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4884.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4971.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4976.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4883.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4978.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4904.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4903.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4936.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4931.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4943.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4992.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4938.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4944.npy\n",
      "Skipping already processed: features/fake/musk-to-taylor_segment_4995.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3394.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3345.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3339.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3393.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3342.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3330.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3337.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3302.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3305.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3379.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3282.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3377.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3370.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3285.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3361.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3294.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3293.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3366.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3314.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3368.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3313.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3326.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3279.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3321.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3353.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3382.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3354.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3385.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3328.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3371.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3284.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3283.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3376.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3304.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3378.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3303.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3336.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3331.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3392.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3343.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3395.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3344.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3338.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3355.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3384.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3329.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3352.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3383.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3278.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3320.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3327.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3312.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3315.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3369.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3292.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3367.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3360.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3295.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3351.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3380.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3356.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3387.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3358.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3389.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3324.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3323.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3316.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3298.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3311.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3363.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3296.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3291.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3318.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3364.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3309.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3280.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3375.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3372.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3287.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3300.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3289.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3307.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3332.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3349.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3335.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3396.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3347.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3391.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3340.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3290.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3319.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3365.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3362.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3297.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3299.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3310.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3317.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3322.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3359.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3388.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3325.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3357.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3386.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3350.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3381.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3390.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3341.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3397.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3346.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3348.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3334.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3333.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3306.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3301.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3288.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3373.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3286.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3308.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3281.npy\n",
      "Skipping already processed: features/fake/obama-to-taylor_segment_3374.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4456.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4487.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4451.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4480.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4384.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4423.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4424.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4383.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4458.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4489.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4411.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4416.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4464.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4418.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4463.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4472.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4475.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4409.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4407.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4400.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4392.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4435.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4498.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4449.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4432.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4395.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4491.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4440.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4496.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4447.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4462.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4465.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4419.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4417.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4410.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4382.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4425.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4459.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4488.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4422.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4385.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4450.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4481.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4457.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4486.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4497.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4446.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4490.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4441.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4394.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4433.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4434.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4393.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4499.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4448.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4401.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4406.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4474.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4408.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4473.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4493.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4442.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4399.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4439.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4494.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4445.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4437.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4390.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4397.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4430.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4479.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4405.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4402.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4470.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4477.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4466.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4461.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4413.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4468.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4414.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4421.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4386.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4381.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4426.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4428.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4454.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4485.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4453.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4482.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4388.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4476.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4471.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4403.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4478.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4404.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4431.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4396.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4391.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4436.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4438.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4495.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4444.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4492.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4443.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4398.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4452.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4483.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4389.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4429.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4455.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4484.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4427.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4380.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4387.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4420.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4469.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4415.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4412.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4460.npy\n",
      "Skipping already processed: features/fake/musk-to-obama_segment_4467.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_374.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_402.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_479.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_373.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_405.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_477.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_470.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_439.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_445.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_442.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_399.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_397.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_430.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_437.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_390.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_381.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_426.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_421.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_386.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_482.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_453.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_388.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_428.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_485.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_454.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_461.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_466.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_468.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_414.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_413.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_391.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_436.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_431.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_396.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_443.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_398.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_438.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_444.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_471.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_476.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_478.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_372.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_404.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_375.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_403.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_412.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_469.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_415.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_467.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_460.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_429.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_484.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_455.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_483.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_452.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_389.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_387.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_420.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_427.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_380.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_416.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_411.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_463.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_464.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_418.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_480.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_451.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_487.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_456.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_424.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_383.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_489.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_458.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_384.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_423.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_432.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_395.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_392.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_435.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_449.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_447.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_440.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_475.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_409.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_378.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_472.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_376.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_400.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_371.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_407.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_422.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_385.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_382.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_425.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_488.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_459.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_486.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_457.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_481.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_450.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_465.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_419.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_462.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_410.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_417.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_406.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_377.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_401.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_379.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_473.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_474.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_408.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_441.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_490.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_446.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_434.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_393.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_448.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_394.npy\n",
      "Skipping already processed: features/fake/taylor-to-biden_segment_433.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2556.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2472.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2475.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2551.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2523.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2558.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2524.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2511.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2498.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2449.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2516.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2518.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2491.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2564.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2563.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2496.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2456.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2487.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2451.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2480.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2509.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2507.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2458.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2489.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2500.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2549.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2535.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2532.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2464.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2540.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2547.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2463.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2562.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2497.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2519.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2490.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2565.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2517.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2510.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2499.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2448.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2559.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2525.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2522.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2474.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2550.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2557.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2473.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2546.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2462.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2465.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2541.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2533.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2548.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2534.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2459.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2488.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2501.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2506.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2450.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2481.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2508.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2457.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2486.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2466.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2542.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2545.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2461.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2539.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2537.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2468.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2530.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2505.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2502.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2454.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2485.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2453.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2482.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2493.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2566.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2561.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2494.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2513.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2514.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2521.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2479.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2526.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2554.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2528.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2470.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2477.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2553.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2452.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2483.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2455.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2484.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2503.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2504.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2469.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2531.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2536.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2544.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2460.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2538.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2467.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2543.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2476.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2552.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2555.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2529.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2471.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2527.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2520.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2478.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2515.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2512.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2560.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2495.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2492.npy\n",
      "Skipping already processed: features/fake/biden-to-ryan_segment_2567.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2971.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3028.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2976.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3021.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2978.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3026.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3013.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2936.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3014.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2931.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2992.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2943.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2995.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2944.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2938.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2955.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2984.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2929.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2952.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2983.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3005.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping already processed: features/fake/biden-to-Obama_segment_2927.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3002.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3037.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3030.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2969.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3042.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2967.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2960.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3039.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2994.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2945.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2939.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2993.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2942.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3015.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2930.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3012.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2937.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3027.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3020.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2979.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2977.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2970.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3029.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2961.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3038.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2966.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3031.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2968.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3036.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2926.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3003.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3004.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2953.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2982.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2954.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2985.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2928.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3040.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2965.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2962.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3035.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3032.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3007.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2959.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2988.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2925.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3000.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2957.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2986.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3009.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2950.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2981.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3018.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2990.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2941.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2997.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2946.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2999.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2948.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3011.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2934.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3016.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2933.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3023.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3024.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2973.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2974.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3008.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2951.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2980.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2956.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2987.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2958.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2989.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2924.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3001.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2923.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3006.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3033.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3034.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2963.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3041.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2964.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2975.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2972.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3025.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3022.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3017.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2932.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2998.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2949.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3010.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2935.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2996.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2947.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_3019.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2991.npy\n",
      "Skipping already processed: features/fake/biden-to-Obama_segment_2940.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2577.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2660.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2570.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2667.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2615.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2669.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2579.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2612.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2627.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2620.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2594.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2652.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2593.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2655.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2629.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2582.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2644.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2638.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2585.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2643.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2631.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2636.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2568.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2603.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2604.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2678.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2676.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2671.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2592.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2654.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2628.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2595.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2653.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2682.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2621.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2626.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2578.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2613.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2614.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2668.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2571.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2666.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2576.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2661.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2670.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2677.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2605.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2679.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2569.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2602.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2637.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2630.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2584.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2642.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2583.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2645.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2639.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2608.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2674.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2673.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2601.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2606.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2633.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2589.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2648.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2634.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2646.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2580.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2641.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2587.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2650.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2596.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2681.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2657.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2591.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2659.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2625.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2622.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2598.npy\n",
      "Skipping already processed: features/fake/linus-to-margot_segment_2617.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2610.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2610.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2575.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2575.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2662.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2662.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2619.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2619.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2572.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2572.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2665.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2665.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2640.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2640.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2586.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2586.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2647.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2647.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2581.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2581.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2649.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2649.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2635.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2635.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2632.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2632.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2588.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2588.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2607.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2607.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2600.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2600.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2672.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2672.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2609.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2609.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2675.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2675.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2618.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2618.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2573.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2573.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2664.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2664.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2574.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2574.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2663.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2663.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2611.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2611.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2616.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2616.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2623.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2623.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2599.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2599.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2658.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2658.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2624.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2624.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2656.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2656.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2590.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2590.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2651.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2651.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2597.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2597.npy\n",
      "Processing: fake_audio/FAKE/linus-to-margot/segment_2680.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/linus-to-margot_segment_2680.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_832.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_832.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_786.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_786.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_849.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_849.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_835.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_835.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_781.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_781.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_847.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_847.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_788.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_788.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_840.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_840.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_809.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_809.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_875.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_875.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_872.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_872.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_800.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_800.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_807.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_807.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_773.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_773.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_816.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_816.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_774.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_774.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_811.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_811.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_863.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_863.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_818.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_818.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_864.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_864.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_799.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_799.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_851.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_851.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_880.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_880.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_856.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_856.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_887.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_887.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_858.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_858.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_824.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_824.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_790.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_790.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_823.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_823.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_797.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_797.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_806.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_806.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_801.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_801.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_873.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_873.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_808.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_808.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_874.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_874.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_789.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_789.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_841.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_841.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_846.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_846.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_848.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_848.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_834.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_834.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_780.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_780.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_833.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_833.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_787.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_787.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_822.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_822.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_796.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_796.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_859.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_859.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_888.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_888.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_825.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_825.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_791.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_791.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_857.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_857.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_886.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_886.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_798.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_798.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_850.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_850.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_881.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_881.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_819.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_819.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_865.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_865.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_862.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_862.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_775.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_775.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_810.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_810.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_772.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_772.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_817.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_817.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_792.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_792.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_826.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_826.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_795.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_795.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_821.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_821.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_853.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_853.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_882.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_882.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_854.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_854.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_885.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_885.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_828.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_828.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_861.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_861.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_778.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_778.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_866.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_866.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_771.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_771.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_814.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_814.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_868.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_868.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_776.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_776.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_813.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_813.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_802.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_802.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_805.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_805.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_879.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_879.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_877.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_877.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_870.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_870.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_769.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_769.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_845.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_845.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_839.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_839.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_842.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_842.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_784.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_784.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_830.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_830.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_783.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_783.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_837.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_837.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_777.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_777.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_812.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_812.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_770.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_770.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_815.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_815.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_869.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_869.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_867.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_867.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_860.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_860.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_779.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_779.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_855.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_855.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_884.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_884.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_829.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_829.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_852.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_852.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_883.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_883.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_794.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_794.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_820.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_820.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_793.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_793.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_827.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_827.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_782.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_782.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_836.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_836.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_785.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_785.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_831.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_831.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_843.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_843.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_844.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_844.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_838.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_838.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_871.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_871.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_876.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_876.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_804.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_804.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_878.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_878.npy\n",
      "Processing: fake_audio/FAKE/trump-to-musk/segment_803.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-musk_segment_803.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4289.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4289.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4300.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4300.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4307.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4307.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4280.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4280.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4309.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4309.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4375.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4375.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4372.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4372.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4287.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4287.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4347.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4347.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4263.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4263.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4264.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4264.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4340.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4340.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4332.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4332.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4349.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4349.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4335.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4335.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4358.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4358.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4324.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4324.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4323.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4323.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4275.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4275.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4351.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4351.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4356.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4356.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4272.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4272.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4363.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4363.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4296.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4296.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4318.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4318.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4291.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4291.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4364.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4364.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4316.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4316.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4311.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4311.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4298.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4298.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4348.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4348.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4334.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4334.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4333.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4333.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4265.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4265.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4341.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4341.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4346.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4346.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4262.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4262.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4373.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4373.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4286.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4286.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4281.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4281.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4308.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4308.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4374.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4374.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4306.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4306.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4288.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4288.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4301.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4301.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4310.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4310.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4299.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4299.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4317.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4317.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4319.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4319.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4290.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4290.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4365.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4365.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4362.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4362.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4297.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4297.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4357.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4357.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4273.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4273.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4274.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4274.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4350.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4350.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4322.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4322.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4359.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4359.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4325.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4325.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4314.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4314.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4368.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4368.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4313.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4313.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4361.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4361.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4294.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4294.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4293.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4293.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4366.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4366.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4277.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4277.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4353.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4353.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4354.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4354.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4328.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4328.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4270.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4270.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4326.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4326.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4321.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4321.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4279.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4279.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4268.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4268.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4330.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4330.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4337.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4337.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4345.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4345.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4261.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4261.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4339.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4339.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4266.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4266.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4342.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4342.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4282.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4282.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4377.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4377.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4370.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4370.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4285.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4285.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4302.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4302.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4305.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4305.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4379.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4379.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4320.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4320.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4278.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4278.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4327.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4327.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4355.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4355.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4329.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4329.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4271.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4271.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4276.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4276.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4352.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4352.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4292.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4292.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4367.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4367.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4360.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4360.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4295.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4295.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4312.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4312.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4315.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4315.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4369.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4369.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4304.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4304.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4378.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4378.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4303.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4303.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4371.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4371.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4284.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4284.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4283.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4283.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4376.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4376.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4267.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4267.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4343.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4343.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4344.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4344.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4260.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4260.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4338.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4338.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4336.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4336.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4269.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4269.npy\n",
      "Processing: fake_audio/FAKE/musk-to-linus/segment_4331.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-linus_segment_4331.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_501.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_501.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_506.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_506.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_508.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_508.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_497.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_497.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_499.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_499.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_509.wav\n",
      "Loaded audio: torch.Size([2, 162729]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_509.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_507.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_507.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_500.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_500.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_498.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_498.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_491.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_491.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_496.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_496.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_495.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_495.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_492.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_492.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_503.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_503.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_504.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_504.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_493.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_493.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_494.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_494.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_505.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_505.npy\n",
      "Processing: fake_audio/FAKE/ryan-to-trump/segment_502.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/ryan-to-trump_segment_502.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1716.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1716.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1711.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1711.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1698.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1698.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1763.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1763.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1696.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1696.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1718.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1718.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1691.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1691.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1764.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1764.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1675.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1675.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1780.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1780.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1751.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1751.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1756.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1756.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1672.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1672.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1758.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1758.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1724.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1724.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1723.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1723.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1732.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1732.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1749.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1749.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1735.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1735.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1747.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1747.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1740.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1740.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1680.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1680.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1709.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1709.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1775.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1775.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1772.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1772.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1687.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1687.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1689.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1689.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1700.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1700.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1707.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1707.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1722.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1722.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1759.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1759.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1725.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1725.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1757.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1757.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1673.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1673.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1674.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1674.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1781.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1781.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1750.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1750.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1719.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1719.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1690.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1690.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1765.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1765.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1762.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1762.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1697.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1697.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1710.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1710.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1699.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1699.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1717.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1717.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1706.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1706.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1688.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1688.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1701.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1701.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1773.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1773.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1686.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1686.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1681.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1681.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1708.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1708.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1774.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1774.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1741.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1741.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1746.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1746.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1748.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1748.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1734.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1734.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1733.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1733.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1702.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1702.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1705.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1705.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1779.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1779.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1682.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1682.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1777.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1777.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1770.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1770.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1685.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1685.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1745.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1745.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1739.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1739.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1666.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1666.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1742.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1742.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1668.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1668.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1730.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1730.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1737.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1737.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1726.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1726.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1721.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1721.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1679.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1679.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1677.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1677.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1782.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1782.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1753.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1753.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1785.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1785.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1754.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1754.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1728.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1728.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1670.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1670.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1761.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1761.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1694.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1694.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1693.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1693.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1766.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1766.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1714.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1714.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1768.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1768.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1713.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1713.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1736.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1736.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1669.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1669.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1731.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1731.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1667.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1667.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1743.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1743.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1744.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1744.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1738.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1738.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1771.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1771.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1684.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1684.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1683.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1683.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1776.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1776.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1704.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1704.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1778.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1778.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1703.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1703.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1712.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1712.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1715.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1715.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1769.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1769.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1692.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1692.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1767.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1767.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1760.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1760.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1695.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1695.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1784.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1784.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1755.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1755.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1729.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1729.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1671.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1671.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1676.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1676.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1783.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1783.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1752.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1752.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1720.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1720.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1678.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1678.npy\n",
      "Processing: fake_audio/FAKE/taylor-to-obama/segment_1727.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/taylor-to-obama_segment_1727.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4010.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4010.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4017.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4017.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4019.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4019.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4065.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4065.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4062.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4062.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4086.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4086.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4057.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4057.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4108.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4108.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4081.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4081.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4050.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4050.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4022.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4022.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4106.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4106.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4101.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4101.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4088.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4088.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4059.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4059.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4025.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4025.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4048.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4048.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4099.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4099.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4110.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4110.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4034.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4034.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4033.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4033.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4117.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4117.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4041.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4041.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4090.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4090.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4119.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4119.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4046.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4046.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4097.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4097.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4073.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4073.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4008.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4008.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4074.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4074.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4006.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4006.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4122.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4122.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4100.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4100.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4089.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4089.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4058.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4058.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4024.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4024.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4023.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4023.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4107.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4107.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4109.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4109.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4080.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4080.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4051.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4051.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4087.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4087.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4056.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4056.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4063.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4063.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4018.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4018.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4064.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4064.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4016.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4016.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4011.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4011.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4007.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4007.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4123.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4123.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4009.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4009.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4075.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4075.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4072.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4072.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4047.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4047.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4096.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4096.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4040.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4040.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4091.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4091.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4118.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4118.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4032.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4032.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4116.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4116.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4049.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4049.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4098.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4098.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4111.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4111.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4035.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4035.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4004.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4004.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4078.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4078.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4120.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4120.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4071.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4071.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4076.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4076.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4043.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4043.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4092.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4092.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4044.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4044.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4095.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4095.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4038.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4038.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4112.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4112.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4036.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4036.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4031.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4031.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4115.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4115.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4020.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4020.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4104.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4104.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4103.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4103.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4027.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4027.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4084.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4084.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4055.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4055.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4029.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4029.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4083.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4083.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4052.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4052.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4067.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4067.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4060.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4060.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4012.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4012.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4015.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4015.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4069.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4069.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4030.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4030.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4114.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4114.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4113.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4113.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4037.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4037.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4045.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4045.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4094.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4094.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4039.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4039.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4042.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4042.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4093.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4093.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4077.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4077.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4070.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4070.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4005.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4005.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4079.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4079.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4121.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4121.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4014.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4014.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4068.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4068.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4013.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4013.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4061.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4061.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4066.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4066.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4082.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4082.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4053.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4053.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4085.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4085.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4054.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4054.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4028.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4028.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4102.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4102.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4026.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4026.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4021.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4021.npy\n",
      "Processing: fake_audio/FAKE/trump-to-ryan/segment_4105.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/trump-to-ryan_segment_4105.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2783.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2783.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2752.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2752.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2729.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2729.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2784.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2784.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2755.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2755.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2727.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2727.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2720.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2720.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2769.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2769.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2715.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2715.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2712.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2712.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2695.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2695.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2760.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2760.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2767.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2767.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2802.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2802.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2692.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2692.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2776.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2776.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2683.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2683.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2684.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2684.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2771.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2771.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2703.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2703.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2778.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2778.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2704.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2704.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2731.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2731.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2736.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2736.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2738.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2738.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2744.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2744.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2795.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2795.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2743.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2743.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2792.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2792.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2766.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2766.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2693.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2693.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2694.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2694.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2761.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2761.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2713.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2713.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2768.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2768.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2714.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2714.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2721.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2721.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2726.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2726.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2728.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2728.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2785.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2785.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2754.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2754.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2782.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2782.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2753.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2753.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2742.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2742.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2793.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2793.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2739.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2739.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2745.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2745.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2794.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2794.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2737.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2737.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2730.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2730.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2779.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2779.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2705.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2705.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2702.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2702.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2685.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2685.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2770.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2770.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2777.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2777.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2746.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2746.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2797.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2797.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2741.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2741.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2790.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2790.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2733.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2733.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2734.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2734.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2748.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2748.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2799.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2799.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2688.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2688.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2701.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2701.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2706.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2706.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2774.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2774.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2708.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2708.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2686.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2686.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2773.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2773.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2697.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2697.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2762.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2762.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2765.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2765.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2800.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2800.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2719.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2719.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2690.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2690.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2717.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2717.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2710.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2710.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2699.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2699.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2725.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2725.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2788.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2788.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2759.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2759.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2722.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2722.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2781.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2781.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2750.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2750.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2786.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2786.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2757.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2757.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2687.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2687.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2772.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2772.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2775.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2775.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2709.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2709.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2707.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2707.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2689.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2689.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2700.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2700.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2735.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2735.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2749.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2749.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2798.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2798.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2732.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2732.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2740.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2740.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2791.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2791.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2747.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2747.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2796.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2796.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2787.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2787.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2756.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2756.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2780.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2780.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2751.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2751.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2723.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2723.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2724.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2724.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2789.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2789.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2758.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2758.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2711.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2711.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2698.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2698.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2716.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2716.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2764.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2764.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2801.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2801.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2718.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2718.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2691.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2691.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2696.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2696.npy\n",
      "Processing: fake_audio/FAKE/biden-to-musk/segment_2763.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/biden-to-musk_segment_2763.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2074.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2074.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2008.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2008.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1975.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1975.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2073.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2073.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2001.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2001.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2006.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2006.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1998.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1998.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2033.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2033.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2034.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2034.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2048.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2048.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1991.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1991.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2046.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2046.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1996.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1996.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2090.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2090.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2041.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2041.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2050.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2050.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1987.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1987.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2081.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2081.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2057.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2057.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1980.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1980.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2086.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2086.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2025.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2025.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2059.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2059.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2088.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2088.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1989.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1989.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2022.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2022.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2017.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2017.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2010.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2010.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2062.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2062.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2065.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2065.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2019.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2019.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1997.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1997.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2091.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2091.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2040.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2040.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1990.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1990.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2047.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2047.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2035.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2035.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2049.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2049.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1999.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1999.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2032.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2032.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2007.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2007.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2000.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2000.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1974.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1974.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2072.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2072.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1973.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1973.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2075.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2075.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2009.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2009.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2064.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2064.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2018.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2018.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2063.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2063.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2011.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2011.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2016.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2016.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1988.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1988.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2023.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2023.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2024.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2024.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2058.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2058.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2089.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2089.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2056.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2056.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1981.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1981.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2087.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2087.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2051.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2051.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1986.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1986.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2080.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2080.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2060.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2060.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2067.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2067.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2069.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2069.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2015.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2015.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2012.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2012.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2027.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2027.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2020.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2020.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2052.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2052.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2083.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2083.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1985.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1985.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2029.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2029.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2055.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2055.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2084.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2084.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1982.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1982.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2038.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2038.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1993.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_1993.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2044.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2044.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_2092.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n",
      "Features extracted successfully\n",
      "Saved features to: features/fake/musk-to-margot_segment_2092.npy\n",
      "Processing: fake_audio/FAKE/musk-to-margot/segment_1994.wav\n",
      "Loaded audio: torch.Size([2, 220500]), Sample rate: 44100\n",
      "Converted to mono\n",
      "Resampling from 44100 to 16000 Hz\n",
      "Prepared input for model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# call function on both real and fake data folders\u001b[39;00m\n\u001b[1;32m     36\u001b[0m process_and_save(real_base, real_out)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mprocess_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_out\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 30\u001b[0m, in \u001b[0;36mprocess_and_save\u001b[0;34m(base_path, out_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# extract features left \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwav_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(save_path, features)\n",
      "Cell \u001b[0;32mIn[37], line 35\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(file_path, max_duration_sec)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# extracting features \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# output: hidden states - averaged over time, produces a single feature vector \u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 35\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     features \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures extracted successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1821\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1816\u001b[0m hidden_states, extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_projection(extract_features)\n\u001b[1;32m   1817\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_hidden_states(\n\u001b[1;32m   1818\u001b[0m     hidden_states, mask_time_indices\u001b[38;5;241m=\u001b[39mmask_time_indices, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask\n\u001b[1;32m   1819\u001b[0m )\n\u001b[0;32m-> 1821\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1829\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1060\u001b[0m, in \u001b[0;36mWav2Vec2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1054\u001b[0m             layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1055\u001b[0m             hidden_states,\n\u001b[1;32m   1056\u001b[0m             attention_mask,\n\u001b[1;32m   1057\u001b[0m             output_attentions,\n\u001b[1;32m   1058\u001b[0m         )\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1060\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:942\u001b[0m, in \u001b[0;36mWav2Vec2EncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    939\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    941\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 942\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[1;32m    945\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:909\u001b[0m, in \u001b[0;36mWav2Vec2FeedForward.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 909\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    910\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    911\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_dropout(hidden_states)\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/wavenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# libraries \n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# input/output paths - where to save features\n",
    "real_base = Path(\"fake_audio/REAL\")\n",
    "fake_base = Path(\"fake_audio/FAKE\")\n",
    "real_out = Path(\"features/real\")\n",
    "fake_out = Path(\"features/fake\")\n",
    "\n",
    "# creating output directories if they don't exist\n",
    "real_out.mkdir(parents=True, exist_ok=True)\n",
    "fake_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# define a processing function with skip logic (easier to pause and resume without having to start over)\n",
    "def process_and_save(base_path, out_path):\n",
    "    for folder in base_path.iterdir():\n",
    "        if folder.is_dir():\n",
    "            for wav_file in folder.glob(\"*.wav\"):\n",
    "                save_name = f\"{folder.name}_{wav_file.name.replace('.wav', '.npy')}\"\n",
    "                save_path = out_path / save_name\n",
    "\n",
    "                # skip if file already processed\n",
    "                if save_path.exists():\n",
    "                    print(f\"Skipping already processed: {save_path}\")\n",
    "                    continue\n",
    "\n",
    "                # extract features left \n",
    "                features = extract_features(str(wav_file))\n",
    "                if features is not None:\n",
    "                    np.save(save_path, features)\n",
    "                    print(f\"Saved features to: {save_path}\")\n",
    "\n",
    "# call function on both real and fake data folders\n",
    "process_and_save(real_base, real_out)\n",
    "process_and_save(fake_base, fake_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and preparing features for modeling \n",
    "# libraries \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# loading all real features (as .npy files)\n",
    "real_features = []\n",
    "for file in Path(\"features/real\").glob(\"*.npy\"):\n",
    "    real_features.append(np.load(file))\n",
    "real_features = np.array(real_features)\n",
    "real_labels = np.zeros(len(real_features))  # 0 = real (target for classification)\n",
    "\n",
    "# loading all fake features\n",
    "fake_features = []\n",
    "for file in Path(\"features/fake\").glob(\"*.npy\"):\n",
    "    fake_features.append(np.load(file))\n",
    "fake_features = np.array(fake_features)\n",
    "fake_labels = np.ones(len(fake_features))  # 1 = fake\n",
    "\n",
    "# combining into one array \n",
    "X = np.vstack([real_features, fake_features])\n",
    "# concatenating labels into single array \n",
    "y = np.concatenate([real_labels, fake_labels])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split \n",
    "\n",
    "* split feature data and labels into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1249 | Test size: 313\n"
     ]
    }
   ],
   "source": [
    "# train test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 80/20 split \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ") # stratify to keep original class distribution (prevent imbalance)\n",
    "\n",
    "print(f\"Train size: {len(X_train)} | Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining MLP Classifier \n",
    "\n",
    "* Defining a Mult-layer Perceptron using PyTorch for binary classification\n",
    "\n",
    "* Simple feed forward neural network suitable for these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining and training MLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=1024, hidden_dim=256):# size of each audio feature vector + number of neurons \n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.ReLU(), # ReLU activation\n",
    "            nn.Dropout(0.3), # regularization\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3), \n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid() # sigmoid activation to squash output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 13.8133\n",
      "Epoch 2, Loss: 13.6616\n",
      "Epoch 3, Loss: 13.4606\n",
      "Epoch 4, Loss: 13.2701\n",
      "Epoch 5, Loss: 13.0833\n",
      "Epoch 6, Loss: 12.7276\n",
      "Epoch 7, Loss: 12.5999\n",
      "Epoch 8, Loss: 12.2581\n",
      "Epoch 9, Loss: 12.2722\n",
      "Epoch 10, Loss: 12.0354\n",
      "Epoch 11, Loss: 11.8338\n",
      "Epoch 12, Loss: 11.7854\n",
      "Epoch 13, Loss: 11.6024\n",
      "Epoch 14, Loss: 11.5284\n",
      "Epoch 15, Loss: 11.3750\n",
      "Epoch 16, Loss: 11.2623\n",
      "Epoch 17, Loss: 11.1732\n",
      "Epoch 18, Loss: 11.2000\n",
      "Epoch 19, Loss: 11.0583\n",
      "Epoch 20, Loss: 10.9276\n",
      "Epoch 21, Loss: 10.8895\n",
      "Epoch 22, Loss: 10.8766\n",
      "Epoch 23, Loss: 10.5583\n",
      "Epoch 24, Loss: 10.5987\n",
      "Epoch 25, Loss: 10.4597\n",
      "Epoch 26, Loss: 10.5913\n",
      "Epoch 27, Loss: 10.4248\n",
      "Epoch 28, Loss: 10.2720\n",
      "Epoch 29, Loss: 10.3581\n",
      "Epoch 30, Loss: 10.5301\n",
      "Epoch 31, Loss: 10.1393\n",
      "Epoch 32, Loss: 10.0483\n",
      "Epoch 33, Loss: 9.8458\n",
      "Epoch 34, Loss: 10.0652\n",
      "Epoch 35, Loss: 9.8185\n",
      "Epoch 36, Loss: 9.9647\n",
      "Epoch 37, Loss: 9.8715\n",
      "Epoch 38, Loss: 9.9460\n",
      "Epoch 39, Loss: 9.6478\n",
      "Epoch 40, Loss: 9.5058\n",
      "Epoch 41, Loss: 9.4316\n",
      "Epoch 42, Loss: 9.5049\n",
      "Epoch 43, Loss: 9.4517\n",
      "Epoch 44, Loss: 9.3120\n",
      "Epoch 45, Loss: 9.3718\n",
      "Epoch 46, Loss: 9.3954\n",
      "Epoch 47, Loss: 9.2970\n",
      "Epoch 48, Loss: 8.9982\n",
      "Epoch 49, Loss: 8.9614\n",
      "Epoch 50, Loss: 9.1163\n",
      "Epoch 51, Loss: 8.9241\n",
      "Epoch 52, Loss: 9.0129\n",
      "Epoch 53, Loss: 8.9483\n",
      "Epoch 54, Loss: 8.8552\n",
      "Epoch 55, Loss: 8.7215\n",
      "Epoch 56, Loss: 8.7335\n",
      "Epoch 57, Loss: 8.9096\n",
      "Epoch 58, Loss: 8.6350\n",
      "Epoch 59, Loss: 8.6805\n",
      "Epoch 60, Loss: 8.5059\n",
      "Epoch 61, Loss: 8.4051\n",
      "Epoch 62, Loss: 8.4520\n",
      "Epoch 63, Loss: 8.2871\n",
      "Epoch 64, Loss: 8.3233\n",
      "Epoch 65, Loss: 8.2741\n",
      "Epoch 66, Loss: 8.1049\n",
      "Epoch 67, Loss: 8.2408\n",
      "Epoch 68, Loss: 8.2166\n",
      "Epoch 69, Loss: 8.1735\n",
      "Epoch 70, Loss: 8.2050\n",
      "Epoch 71, Loss: 8.1372\n",
      "Epoch 72, Loss: 8.2009\n",
      "Epoch 73, Loss: 7.8574\n",
      "Epoch 74, Loss: 7.8330\n",
      "Epoch 75, Loss: 7.7640\n",
      "Epoch 76, Loss: 7.6819\n",
      "Epoch 77, Loss: 7.6018\n",
      "Epoch 78, Loss: 7.6835\n",
      "Epoch 79, Loss: 7.6542\n",
      "Epoch 80, Loss: 7.5509\n",
      "Epoch 81, Loss: 7.5958\n",
      "Epoch 82, Loss: 7.4158\n",
      "Epoch 83, Loss: 7.5384\n",
      "Epoch 84, Loss: 7.4259\n",
      "Epoch 85, Loss: 7.1716\n",
      "Epoch 86, Loss: 7.2922\n",
      "Epoch 87, Loss: 7.2728\n",
      "Epoch 88, Loss: 7.1100\n",
      "Epoch 89, Loss: 7.0200\n",
      "Epoch 90, Loss: 7.1716\n",
      "Epoch 91, Loss: 6.9178\n",
      "Epoch 92, Loss: 6.8824\n",
      "Epoch 93, Loss: 6.7932\n",
      "Epoch 94, Loss: 6.6358\n",
      "Epoch 95, Loss: 6.7257\n",
      "Epoch 96, Loss: 6.7054\n",
      "Epoch 97, Loss: 6.8902\n",
      "Epoch 98, Loss: 6.5528\n",
      "Epoch 99, Loss: 6.3866\n",
      "Epoch 100, Loss: 6.8147\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# converting features/labels into pytorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# wraps training tensors - loaded in batches\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# input size matching feature vector \n",
    "model = MLP(input_dim=X.shape[1])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# adam \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.BCELoss() # binary cross entropy loss - binary classification\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "        # gradients reset\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(batch_X)\n",
    "        loss = criterion(preds, batch_y)\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating MLP Classifier \n",
    "* evaluating how well trained MLP performs on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Real       0.00      0.00      0.00       153\n",
      "        Fake       0.51      1.00      0.68       160\n",
      "\n",
      "    accuracy                           0.51       313\n",
      "   macro avg       0.26      0.50      0.34       313\n",
      "weighted avg       0.26      0.51      0.35       313\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/stor/home/spl742/wavenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/stor/home/spl742/wavenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/stor/home/spl742/wavenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# converting X_test to torch.Tensor and moved to device\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# run inference\n",
    "with torch.no_grad(): # disabling gradient tracking\n",
    "    test_preds = model(X_test_tensor)\n",
    "    predicted_labels = torch.round(torch.sigmoid(test_preds)).squeeze()\n",
    "\n",
    "# print classification report\n",
    "print(classification_report(y_test, predicted_labels.cpu(), target_names=[\"Real\", \"Fake\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train distribution: Counter({np.float64(1.0): 640, np.float64(0.0): 609})\n"
     ]
    }
   ],
   "source": [
    "# count class distribution in training set\n",
    "from collections import Counter\n",
    "print(\"Train distribution:\", Counter(y_train))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (wavenv)",
   "language": "python",
   "name": "wavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00eb0b64542542318388f6fd723fb817": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d54b65f3c1246abb286025de86234bb",
       "IPY_MODEL_89873e1a1d334471bbb34ad79b6cdcfa",
       "IPY_MODEL_bd2ecfecedce4a59a06bd2318cade3f0"
      ],
      "layout": "IPY_MODEL_1e9ab87310ab4812ad2ffce24b9fb04b"
     }
    },
    "04b51efe95374941b050d9442422317c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1348871bd3c444bfbef809926d93ab83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04b51efe95374941b050d9442422317c",
      "placeholder": "​",
      "style": "IPY_MODEL_4e3f6de6e3a34a2d89be04bcf0b58a16",
      "value": " 163/163 [00:00&lt;00:00, 12.3kB/s]"
     }
    },
    "134e9f3c4af943df92af2918f1deb49f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13a89e8dcf8f43a29f12a5ec2aaef281": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "181be9bd7cd34bab8ff8695be572004b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e1e4b3e15f24e94b65e5dc57782d6cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e9ab87310ab4812ad2ffce24b9fb04b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "215160d9377d4f5897c791a46e081d20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23cee12ea96f413dbd870d4956fa1156": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "25d8a942d1ff474c8868e07a8c147e71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_421da2074c284b4bb020aca04a629966",
      "placeholder": "​",
      "style": "IPY_MODEL_b9841d167b5349fdb7812904c5d9c6e6",
      "value": " 159/159 [00:00&lt;00:00, 11.4kB/s]"
     }
    },
    "2bbde76e970a4c029d8c1357e8bb1e67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6cdee6337114fd1bfd996a7a5697c2e",
      "max": 85,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_511812f8bf344a33a16e82fd7f707b11",
      "value": 85
     }
    },
    "3843ef0bef1f48bbb3d58f568a8e337a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb2ade7a5db2472da0e81415b27bc62c",
       "IPY_MODEL_7b47ec10c56d4b4eadeda4022f431e76",
       "IPY_MODEL_8fa1cb9803104d129d4e74e84441bff6"
      ],
      "layout": "IPY_MODEL_816f9f10f9ce47e981f204d27d74a030"
     }
    },
    "3a32652440fa4e9390197eaa1b64996d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bc5a683b8844dce9d320417bc32705a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b14c3368f08e4886ac2470f39cddb9ae",
      "placeholder": "​",
      "style": "IPY_MODEL_8504530fd65545c9b879e3daaf6f16db",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "3ff6faf88da041539593be3843db7808": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4142d7c8cac240e09b96b64d560fbb05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "421da2074c284b4bb020aca04a629966": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "42ea0ff0d139466d898d7afdf4a005a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "453c6e5664aa4ff8842f6d694d9c4ebb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efb670cde16b4203ae4968c79e793e3a",
      "max": 1261893126,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6749ea46e2fe4f8c863c42ad99653979",
      "value": 1261893126
     }
    },
    "473fd9fbceb347d683be14b15e00a4ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "495c0f0f12e040a6b24eb310fc3330ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bc9769fb6bd4e1eb5eb09393be3e493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_495c0f0f12e040a6b24eb310fc3330ef",
      "placeholder": "​",
      "style": "IPY_MODEL_181be9bd7cd34bab8ff8695be572004b",
      "value": " 1.26G/1.26G [00:10&lt;00:00, 53.5MB/s]"
     }
    },
    "4e3f6de6e3a34a2d89be04bcf0b58a16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "511812f8bf344a33a16e82fd7f707b11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "550e8dff462646ec9baf7dd475a51a91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5c0212279cf49119fee6e6730c6898d",
      "max": 291,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e801e6fd2f0c40e0a42da25df8bdce48",
      "value": 291
     }
    },
    "5e8e0ec9fe9241bcbef69a0413f724aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b89115d4972e4ba1b63acdca4c4f3c42",
      "placeholder": "​",
      "style": "IPY_MODEL_66d0dee9199346a48f22e47cfca72bf2",
      "value": " 291/291 [00:00&lt;00:00, 23.6kB/s]"
     }
    },
    "668a7aa0ccaa47a5920f360539fcad53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f74da3b8aa1b463d8dc1b990412826d2",
      "placeholder": "​",
      "style": "IPY_MODEL_b4713bbac0b04b88b7bbbcd153304be8",
      "value": "vocab.json: 100%"
     }
    },
    "66d0dee9199346a48f22e47cfca72bf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6740692c37e441c581ce1dd14e1040a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6749ea46e2fe4f8c863c42ad99653979": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6d4f022532e548c7bbc7384487b7a261": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d54b65f3c1246abb286025de86234bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6740692c37e441c581ce1dd14e1040a0",
      "placeholder": "​",
      "style": "IPY_MODEL_cb686a84b354411fbeab52569b3971cc",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "74b21d9f52de43cd838cdc8e47d1efbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4473af4b51042a2a11559be09c923c0",
      "max": 159,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_852d1c600df34da094a3ef3fbff6a9a1",
      "value": 159
     }
    },
    "78d968bbed6b47dfb164a9392e0dc2af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3bc5a683b8844dce9d320417bc32705a",
       "IPY_MODEL_88297e4bad5a4ab7ba9530c87c468e66",
       "IPY_MODEL_1348871bd3c444bfbef809926d93ab83"
      ],
      "layout": "IPY_MODEL_13a89e8dcf8f43a29f12a5ec2aaef281"
     }
    },
    "7b47ec10c56d4b4eadeda4022f431e76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cb56bbe7ad84a868a9ed2f717d5b9c9",
      "max": 843,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23cee12ea96f413dbd870d4956fa1156",
      "value": 843
     }
    },
    "816f9f10f9ce47e981f204d27d74a030": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8504530fd65545c9b879e3daaf6f16db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "85119e6cc76646a0b15ba87786581ac1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_215160d9377d4f5897c791a46e081d20",
      "placeholder": "​",
      "style": "IPY_MODEL_99595b23811542a6a65296c6f4918f00",
      "value": "model.safetensors: 100%"
     }
    },
    "852d1c600df34da094a3ef3fbff6a9a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "85a5b97e367241ee9164549eb2c00c54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_668a7aa0ccaa47a5920f360539fcad53",
       "IPY_MODEL_550e8dff462646ec9baf7dd475a51a91",
       "IPY_MODEL_5e8e0ec9fe9241bcbef69a0413f724aa"
      ],
      "layout": "IPY_MODEL_c65a98cd618742eaa89586c5654162c0"
     }
    },
    "88297e4bad5a4ab7ba9530c87c468e66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ff6faf88da041539593be3843db7808",
      "max": 163,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0f405e4c8a44214a70a988ea140ca88",
      "value": 163
     }
    },
    "88c8d693dd5f41739b3ef6e502ee4ba9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89873e1a1d334471bbb34ad79b6cdcfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d4f022532e548c7bbc7384487b7a261",
      "max": 1262009187,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9dc9c7ed9f84327b71277ce977e4964",
      "value": 1262009187
     }
    },
    "8b4d12dd84d8474eb9f1e0268f8820d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d47e87675144580aaf6be5581d5e148": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fa1cb9803104d129d4e74e84441bff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4142d7c8cac240e09b96b64d560fbb05",
      "placeholder": "​",
      "style": "IPY_MODEL_42ea0ff0d139466d898d7afdf4a005a6",
      "value": " 843/843 [00:00&lt;00:00, 72.5kB/s]"
     }
    },
    "933a03fda51a490ca3f3d5fdcd482848": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99003d3df5694b01bd2180ebf1f4b74a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85119e6cc76646a0b15ba87786581ac1",
       "IPY_MODEL_453c6e5664aa4ff8842f6d694d9c4ebb",
       "IPY_MODEL_4bc9769fb6bd4e1eb5eb09393be3e493"
      ],
      "layout": "IPY_MODEL_134e9f3c4af943df92af2918f1deb49f"
     }
    },
    "99595b23811542a6a65296c6f4918f00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9cb56bbe7ad84a868a9ed2f717d5b9c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a129ca36c1a04cf0aa4ffcd5819c2d8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_acce6567858d4f88b394dccce3b1b7c1",
       "IPY_MODEL_2bbde76e970a4c029d8c1357e8bb1e67",
       "IPY_MODEL_cc0d2f6b099f4471b011ce79b8ddee94"
      ],
      "layout": "IPY_MODEL_1e1e4b3e15f24e94b65e5dc57782d6cb"
     }
    },
    "acce6567858d4f88b394dccce3b1b7c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2a15925f919418db7d676c04d065cd2",
      "placeholder": "​",
      "style": "IPY_MODEL_baea604bc6914e0a8856195b2ba6f692",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "b14c3368f08e4886ac2470f39cddb9ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2a15925f919418db7d676c04d065cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4473af4b51042a2a11559be09c923c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4713bbac0b04b88b7bbbcd153304be8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b572b503235d4662a247b57c56b0af87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5c0212279cf49119fee6e6730c6898d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b89115d4972e4ba1b63acdca4c4f3c42": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9841d167b5349fdb7812904c5d9c6e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "baea604bc6914e0a8856195b2ba6f692": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd2ecfecedce4a59a06bd2318cade3f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88c8d693dd5f41739b3ef6e502ee4ba9",
      "placeholder": "​",
      "style": "IPY_MODEL_8d47e87675144580aaf6be5581d5e148",
      "value": " 1.26G/1.26G [00:12&lt;00:00, 99.7MB/s]"
     }
    },
    "c65a98cd618742eaa89586c5654162c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb2ade7a5db2472da0e81415b27bc62c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5d3dc2f4ecb4388ac82ac628d27e5c1",
      "placeholder": "​",
      "style": "IPY_MODEL_8b4d12dd84d8474eb9f1e0268f8820d8",
      "value": "config.json: 100%"
     }
    },
    "cb686a84b354411fbeab52569b3971cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc0d2f6b099f4471b011ce79b8ddee94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_933a03fda51a490ca3f3d5fdcd482848",
      "placeholder": "​",
      "style": "IPY_MODEL_dd2542a6f80d49dea2d17c0a55c7594f",
      "value": " 85.0/85.0 [00:00&lt;00:00, 4.55kB/s]"
     }
    },
    "d6cdee6337114fd1bfd996a7a5697c2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd2542a6f80d49dea2d17c0a55c7594f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de7b2fcc15a9495981c376f52d7d4ed2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_473fd9fbceb347d683be14b15e00a4ca",
      "placeholder": "​",
      "style": "IPY_MODEL_b572b503235d4662a247b57c56b0af87",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "e0f405e4c8a44214a70a988ea140ca88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e5d3dc2f4ecb4388ac82ac628d27e5c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e801e6fd2f0c40e0a42da25df8bdce48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e9dc9c7ed9f84327b71277ce977e4964": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efb670cde16b4203ae4968c79e793e3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0b627b35c27489b930b1af1fe56dad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de7b2fcc15a9495981c376f52d7d4ed2",
       "IPY_MODEL_74b21d9f52de43cd838cdc8e47d1efbc",
       "IPY_MODEL_25d8a942d1ff474c8868e07a8c147e71"
      ],
      "layout": "IPY_MODEL_3a32652440fa4e9390197eaa1b64996d"
     }
    },
    "f74da3b8aa1b463d8dc1b990412826d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
